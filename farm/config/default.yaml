# Default configuration for Agent Farm Simulation Framework
# This file contains all base configuration settings with sensible defaults

# Environment settings
width: 100
height: 100

# Position discretization settings
position_discretization_method: "floor"  # Options: "floor", "round", "ceil"
use_bilinear_interpolation: true  # Whether to use bilinear interpolation for resources

# Agent settings
system_agents: 10
independent_agents: 10
control_agents: 10
initial_resource_level: 5
max_population: 300
starvation_threshold: 10
offspring_cost: 3
min_reproduction_resources: 8
offspring_initial_resources: 5
perception_radius: 2
base_attack_strength: 2
base_defense_strength: 2
seed: 1234567890

# Agent type ratios (must sum to 1.0)
agent_type_ratios:
  SystemAgent: 0.33
  IndependentAgent: 0.33
  ControlAgent: 0.34

# Resource settings
initial_resources: 20
resource_regen_rate: 0.1
resource_regen_amount: 2
max_resource_amount: 30
default_spawn_amount: 5

# Agent behavior settings
base_consumption_rate: 0.15
max_movement: 8
gathering_range: 30
max_gather_amount: 3
territory_range: 30

# General Learning parameters
learning_rate: 0.001
gamma: 0.95
epsilon_start: 1.0
epsilon_min: 0.01
epsilon_decay: 0.995
memory_size: 2000
batch_size: 32
training_frequency: 4
dqn_hidden_size: 24
tau: 0.005
dqn_state_cache_size: 100
gradient_clip_norm: 1.0
enable_gradient_clipping: true

# Movement Module Parameters
move_target_update_freq: 100
move_memory_size: 10000
move_learning_rate: 0.001
move_gamma: 0.99
move_epsilon_start: 1.0
move_epsilon_min: 0.01
move_epsilon_decay: 0.995
move_dqn_hidden_size: 64
move_batch_size: 32
move_reward_history_size: 100
move_epsilon_adapt_threshold: 0.1
move_epsilon_adapt_factor: 1.5
move_min_reward_samples: 10
move_tau: 0.005
move_base_cost: -0.1
move_resource_approach_reward: 0.3
move_resource_retreat_penalty: -0.2

# Attack Module Parameters
attack_target_update_freq: 100
attack_memory_size: 10000
attack_learning_rate: 0.001
attack_gamma: 0.99
attack_epsilon_start: 1.0
attack_epsilon_min: 0.01
attack_epsilon_decay: 0.995
attack_dqn_hidden_size: 64
attack_batch_size: 32
attack_tau: 0.005
attack_base_cost: -0.2
attack_success_reward: 1.0
attack_failure_penalty: -0.3
attack_defense_threshold: 0.3
attack_defense_boost: 2.0
attack_kill_reward: 5.0

# Combat Parameters
starting_health: 100.0
attack_range: 20.0
attack_base_damage: 10.0

# Action Multipliers
attack_mult_desperate: 1.4
attack_mult_stable: 0.6
attack_starvation_threshold: 0.5

# Gathering Parameters
gather_success_reward: 0.5
gather_failure_penalty: -0.1
gather_base_cost: -0.05
gather_target_update_freq: 100
gather_memory_size: 10000
gather_learning_rate: 0.001
gather_gamma: 0.99
gather_epsilon_start: 1.0
gather_epsilon_min: 0.01
gather_epsilon_decay: 0.995
gather_dqn_hidden_size: 64
gather_batch_size: 32
gather_tau: 0.005
gather_distance_penalty_factor: 0.1
gather_resource_threshold: 0.2
gather_competition_penalty: -0.2
gather_efficiency_bonus: 0.3

# Sharing Parameters
share_range: 30.0
share_target_update_freq: 100
share_memory_size: 10000
share_learning_rate: 0.001
share_gamma: 0.99
share_epsilon_start: 1.0
share_epsilon_min: 0.01
share_epsilon_decay: 0.995
share_dqn_hidden_size: 64
share_batch_size: 32
share_tau: 0.005
share_success_reward: 0.5
share_failure_penalty: -0.1
share_base_cost: -0.05
min_share_amount: 1
max_share_amount: 5
share_threshold: 0.3
share_cooperation_bonus: 0.2
share_altruism_factor: 1.2
cooperation_memory: 100
cooperation_score_threshold: 0.5

# Agent-specific parameters
agent_parameters:
  SystemAgent:
    gather_efficiency_multiplier: 0.4
    gather_cost_multiplier: 0.4
    min_resource_threshold: 0.2
    share_weight: 0.3
    attack_weight: 0.05
    cooperation_threshold: 0.5
    learning_rate: 0.01
    exploration_rate: 0.1
  IndependentAgent:
    gather_efficiency_multiplier: 0.7
    gather_cost_multiplier: 0.2
    min_resource_threshold: 0.05
    share_weight: 0.05
    attack_weight: 0.25
    cooperation_threshold: 0.5
    learning_rate: 0.01
    exploration_rate: 0.1
  ControlAgent:
    gather_efficiency_multiplier: 0.55
    gather_cost_multiplier: 0.3
    min_resource_threshold: 0.125
    share_weight: 0.15
    attack_weight: 0.15
    cooperation_threshold: 0.5
    learning_rate: 0.01
    exploration_rate: 0.1

# Action probability adjustment parameters
social_range: 30

# Movement multipliers
move_mult_no_resources: 1.5

# Gathering multipliers
gather_mult_low_resources: 1.5

# Sharing multipliers
share_mult_wealthy: 1.3
share_mult_poor: 0.5

# Simulation control
max_wait_steps: 10
simulation_steps: 100
max_steps: 1000

# Database configuration
use_in_memory_db: false
persist_db_on_completion: true
in_memory_db_memory_limit_mb: null
in_memory_tables_to_persist: null

# Database pragma settings
db_pragma_profile: "balanced"
db_cache_size_mb: 200
db_synchronous_mode: "NORMAL"
db_journal_mode: "WAL"
db_custom_pragmas: {}

# Database connection pooling
connection_pool_size: 10
connection_pool_recycle: 3600
connection_timeout: 30

# Database buffering and commits
log_buffer_size: 1000
commit_interval_seconds: 30

# Database export settings
export_batch_size: 1000

# Database validation settings
enable_validation: true
validation_include_integrity: true
validation_include_statistical: true

# Curriculum learning phases
curriculum_phases:
  - steps: 100
    enabled_actions: ["move", "gather"]
  - steps: 200
    enabled_actions: ["move", "gather", "share", "attack"]
  - steps: -1
    enabled_actions: ["move", "gather", "share", "attack", "reproduce"]

# Device configuration for neural network computations
device_preference: "auto"
device_fallback: true
device_memory_fraction: null
device_validate_compatibility: true

# Performance optimization settings
agent_processing_batch_size: 32
resource_processing_batch_size: 100
enable_parallel_processing: false
max_worker_threads: 4
enable_memory_pooling: true
memory_pool_size_mb: 100
enable_state_caching: true
cache_ttl_seconds: 60

# Action reward configuration
defend_success_reward: 0.02
pass_action_reward: 0.01
successful_gather_bonus: 0.05
successful_share_bonus: 0.03
successful_attack_bonus: 0.1
reproduction_success_bonus: 0.15
failed_action_penalty: -0.05
collision_penalty: -0.02

# Redis configuration
redis:
  host: localhost
  port: 6379
  db: 0
  password: null
  decode_responses: true
  environment: default

# Visualization settings
visualization:
  canvas_size: [400, 400]
  padding: 20
  background_color: "black"
  max_animation_frames: 5
  animation_min_delay: 50
  max_resource_amount: 30
  resource_colors:
    glow_red: 50
    glow_green: 255
    glow_blue: 50
  resource_size: 2
  agent_radius_scale: 2
  birth_radius_scale: 4
  death_mark_scale: 1.5
  agent_colors:
    SystemAgent: "blue"
    IndependentAgent: "red"
    ControlAgent: "#DAA520"
  min_font_size: 10
  font_scale_factor: 40
  font_family: "arial"
  death_mark_color: [255, 0, 0]
  birth_mark_color: [255, 255, 255]
  metric_colors:
    total_agents: "#4a90e2"
    system_agents: "#50c878"
    independent_agents: "#e74c3c"
    control_agents: "#DAA520"
    total_resources: "#f39c12"
    average_agent_resources: "#9b59b6"

# Analysis configurations
spatial_analysis:
  resource_clustering_threshold: 20.0
  gathering_range: 30.0
  hotspot_threshold_stds: 1.0
  max_clusters: 10
  min_clustering_points: 3
  density_bins: 20

genesis_analysis:
  resource_proximity_threshold: 30.0
  critical_period_end: 100
  auc_weight: 0.2
  recency_weighted_auc_weight: 0.3
  dominance_duration_weight: 0.2
  growth_trend_weight: 0.1
  final_ratio_weight: 0.2

agent_analysis:
  learning_curve_window: 10
  behavior_clusters: 3
  success_rate_weight: 0.4
  reward_rate_weight: 0.4
  lifespan_weight: 0.2
  top_performers_count: 5

population_analysis:
  stability_window: 50
  growth_window: 20

learning_analysis:
  moving_average_window: 10
  convergence_window: 20

analysis_global:
  default_db_filename: "simulation.db"
  output_subdirs: ["plots", "data", "reports"]
  plot_dpi: 300
  plot_style: "default"
  default_figsize: [8, 6]
  verbose_logging: false
  enable_caching: true
  enable_parallel: false
  max_workers: null
