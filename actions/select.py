"""Action selection module for intelligent action prioritization.

This module provides a flexible framework for agents to make intelligent decisions
about which action to take during their turn, considering:
- Current state and environment
- Action weights and probabilities
- State-based adjustments
- Exploration vs exploitation

The module uses a combination of predefined weights and learned preferences to
select optimal actions for different situations.
"""

import logging
import random
from typing import TYPE_CHECKING, List

import numpy as np
import torch

from action import Action
from actions.base_dqn import BaseDQNConfig, BaseDQNModule, BaseQNetwork

if TYPE_CHECKING:
    from agents.base_agent import BaseAgent

logger = logging.getLogger(__name__)


class SelectConfig(BaseDQNConfig):
    """Configuration for action selection behavior."""

    # Base action weights
    move_weight: float = 0.4
    gather_weight: float = 0.3
    share_weight: float = 0.2
    attack_weight: float = 0.1

    # State-based multipliers
    move_mult_no_resources: float = 1.5
    gather_mult_low_resources: float = 1.5
    share_mult_wealthy: float = 1.3
    share_mult_poor: float = 0.5
    attack_mult_desperate: float = 1.4
    attack_mult_stable: float = 0.6

    # Thresholds
    attack_starvation_threshold: float = 0.5
    attack_defense_threshold: float = 0.3


class SelectQNetwork(BaseQNetwork):
    """Neural network for action selection decisions."""

    def __init__(self, input_dim: int, num_actions: int, hidden_size: int = 64) -> None:
        super().__init__(input_dim, num_actions, hidden_size)


class SelectModule(BaseDQNModule):
    """Module for learning and executing action selection."""

    def __init__(
        self,
        num_actions: int,
        config: SelectConfig,
        device: torch.device = torch.device(
            "cuda" if torch.cuda.is_available() else "cpu"
        ),
    ) -> None:
        super().__init__(
            input_dim=8,  # State dimensions for selection
            output_dim=num_actions,
            config=config,
            device=device,
        )

    def select_action(
        self, agent: "BaseAgent", actions: List[Action], state: torch.Tensor
    ) -> Action:
        """Select an action using both predefined weights and learned preferences.

        Args:
            agent: Agent making the decision
            actions: List of available actions
            state: Current state tensor

        Returns:
            Selected Action object
        """
        # Get base probabilities from weights
        base_probs = [action.weight for action in actions]

        # Adjust probabilities based on state
        adjusted_probs = self._adjust_probabilities(agent, base_probs)

        # Use epsilon-greedy for exploration
        if random.random() < self.epsilon:
            return random.choices(actions, weights=adjusted_probs, k=1)[0]

        # Get Q-values from network
        with torch.no_grad():
            q_values = self.q_network(state)

        # Combine Q-values with adjusted probabilities
        combined_probs = self._combine_probs_and_qvalues(
            adjusted_probs, q_values.cpu().numpy()
        )

        return random.choices(actions, weights=combined_probs, k=1)[0]

    def _adjust_probabilities(
        self, agent: "BaseAgent", base_probs: List[float]
    ) -> List[float]:
        """Adjust action probabilities based on agent's current state."""
        adjusted_probs = base_probs.copy()
        config = self.config

        # Get state information
        resource_level = agent.resource_level
        starvation_risk = agent.starvation_threshold / agent.max_starvation
        health_ratio = agent.current_health / agent.max_health

        # Find nearby entities
        nearby_resources = [
            r
            for r in agent.environment.resources
            if not r.is_depleted()
            and np.sqrt(((np.array(r.position) - np.array(agent.position)) ** 2).sum())
            < agent.config.gathering_range
        ]

        nearby_agents = [
            a
            for a in agent.environment.agents
            if a != agent
            and a.alive
            and np.sqrt(((np.array(a.position) - np.array(agent.position)) ** 2).sum())
            < agent.config.social_range
        ]

        # Adjust move probability
        move_idx = next(i for i, a in enumerate(agent.actions) if a.name == "move")
        if not nearby_resources:
            adjusted_probs[move_idx] *= config.move_mult_no_resources

        # Adjust gather probability
        gather_idx = next(i for i, a in enumerate(agent.actions) if a.name == "gather")
        if (
            nearby_resources
            and resource_level < agent.config.min_reproduction_resources
        ):
            adjusted_probs[gather_idx] *= config.gather_mult_low_resources

        # Adjust share probability
        share_idx = next(i for i, a in enumerate(agent.actions) if a.name == "share")
        if resource_level > agent.config.min_reproduction_resources and nearby_agents:
            adjusted_probs[share_idx] *= config.share_mult_wealthy
        else:
            adjusted_probs[share_idx] *= config.share_mult_poor

        # Adjust attack probability
        attack_idx = next(i for i, a in enumerate(agent.actions) if a.name == "attack")
        if (
            starvation_risk > config.attack_starvation_threshold
            and nearby_agents
            and resource_level > 2
        ):
            adjusted_probs[attack_idx] *= config.attack_mult_desperate
        else:
            adjusted_probs[attack_idx] *= config.attack_mult_stable

        if health_ratio < config.attack_defense_threshold:
            adjusted_probs[attack_idx] *= 0.5
        elif (
            health_ratio > 0.8
            and resource_level > agent.config.min_reproduction_resources
        ):
            adjusted_probs[attack_idx] *= 1.5

        # Normalize probabilities
        total = sum(adjusted_probs)
        return [p / total for p in adjusted_probs]

    def _combine_probs_and_qvalues(
        self, probs: List[float], q_values: np.ndarray
    ) -> List[float]:
        """Combine adjusted probabilities with Q-values."""
        # Normalize Q-values to [0,1] range
        q_normalized = (q_values - q_values.min()) / (
            q_values.max() - q_values.min() + 1e-8
        )

        # Combine using weighted average
        combined = 0.7 * np.array(probs) + 0.3 * q_normalized

        # Normalize
        return combined / combined.sum()


def create_selection_state(agent: "BaseAgent") -> torch.Tensor:
    """Create state representation for action selection."""
    # Calculate normalized values
    max_resources = agent.config.min_reproduction_resources * 3
    resource_ratio = agent.resource_level / max_resources
    health_ratio = agent.current_health / agent.max_health
    starvation_ratio = agent.starvation_threshold / agent.max_starvation

    # Get nearby entities
    nearby_resources = len(
        [
            r
            for r in agent.environment.resources
            if not r.is_depleted()
            and np.sqrt(((np.array(r.position) - np.array(agent.position)) ** 2).sum())
            < agent.config.gathering_range
        ]
    )

    nearby_agents = len(
        [
            a
            for a in agent.environment.agents
            if a != agent
            and a.alive
            and np.sqrt(((np.array(a.position) - np.array(agent.position)) ** 2).sum())
            < agent.config.social_range
        ]
    )

    # Create state tensor - removed time/max_steps ratio since max_steps isn't available
    state = torch.tensor(
        [
            resource_ratio,
            health_ratio,
            starvation_ratio,
            nearby_resources / max(1, len(agent.environment.resources)),
            nearby_agents / max(1, len(agent.environment.agents)),
            float(
                agent.environment.time > 0
            ),  # Simple binary indicator if not first step
            float(agent.is_defending),
            float(agent.alive),
        ],
        dtype=torch.float32,
        device=agent.device,
    )

    return state
