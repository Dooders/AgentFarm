name: Database Performance Baseline

on:
  pull_request:
    branches: [ main ]
    types: [ opened, synchronize, reopened ]
  push:
    branches: [ main ]

jobs:
  database-performance-baseline:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Create results directory
      run: |
        mkdir -p benchmarks/results/database_profiling
        
    - name: Run database performance benchmark
      run: |
        python -m benchmarks.run_benchmarks --spec benchmarks/specs/memory_db_baseline.yaml
        
    - name: Upload profiling results
      uses: actions/upload-artifact@v4
      with:
        name: database-profiling-results-${{ github.run_number }}
        path: benchmarks/results/database_profiling/
        retention-days: 30
        
    - name: Generate performance summary
      run: |
        echo "## Database Performance Baseline Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Latest Performance Metrics" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Extract key metrics from the latest benchmark results
        LATEST_RESULTS=$(ls -t benchmarks/results/database_profiling/memory_db_*.json | head -1)
        if [ -f "$LATEST_RESULTS" ]; then
          echo "```" >> $GITHUB_STEP_SUMMARY
          echo "Database Performance Results:" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          # Extract key metrics using jq if available, otherwise show file info
          if command -v jq &> /dev/null; then
            jq -r '.metrics | to_entries[] | "\(.key): \(.value)"' "$LATEST_RESULTS" >> $GITHUB_STEP_SUMMARY
          else
            echo "Results file: $(basename "$LATEST_RESULTS")" >> $GITHUB_STEP_SUMMARY
            echo "Check artifacts for detailed results" >> $GITHUB_STEP_SUMMARY
          fi
          echo "```" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Performance Artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Complete profiling results are available in the workflow artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Results include JSON, CSV, and summary report formats" >> $GITHUB_STEP_SUMMARY
        echo "- Use these results to track performance trends over time" >> $GITHUB_STEP_SUMMARY
        
    - name: Check for performance regressions
      run: |
        echo "Checking for significant performance regressions..."
        
        # Load the latest results and check against thresholds
        python3 << 'EOF'
        import json
        import glob
        import os
        import sys
        from pathlib import Path
        from typing import Any, Dict, List, Optional, Union
        
        def safe_get_nested(data: Dict[str, Any], *keys: str, default: Any = None) -> Any:
            """Safely get nested dictionary values with fallback."""
            try:
                current = data
                for key in keys:
                    if isinstance(current, dict) and key in current:
                        current = current[key]
                    else:
                        return default
                return current
            except (TypeError, KeyError, AttributeError):
                return default
        
        def extract_metric_value(metrics: Dict[str, Any], metric_path: str, default: float = 0.0) -> float:
            """Extract metric value from various possible structures."""
            try:
                # Handle direct metric access
                if metric_path in metrics:
                    value = metrics[metric_path]
                    if isinstance(value, (int, float)):
                        return float(value)
                    elif isinstance(value, dict) and 'mean' in value:
                        return float(value['mean'])
                    elif isinstance(value, dict) and 'value' in value:
                        return float(value['value'])
                
                # Handle nested metric access (e.g., "duration_s.mean")
                if '.' in metric_path:
                    parts = metric_path.split('.')
                    return safe_get_nested(metrics, *parts, default=default)
                
                return default
            except (TypeError, ValueError, KeyError):
                return default
        
        def validate_json_structure(data: Dict[str, Any]) -> bool:
            """Validate that the JSON has the expected structure."""
            required_fields = ['name', 'run_id', 'parameters']
            return all(field in data for field in required_fields)
        
        def find_latest_results() -> Optional[Path]:
            """Find the latest benchmark results file."""
            results_dir = Path("benchmarks/results/database_profiling")
            if not results_dir.exists():
                print(f"Results directory does not exist: {results_dir}")
                return None
            
            # Look for various possible file patterns
            patterns = ["memory_db_*.json", "*.json"]
            json_files = []
            
            for pattern in patterns:
                json_files.extend(results_dir.glob(pattern))
            
            if not json_files:
                print("No benchmark results found")
                return None
            
            # Get the most recent file by modification time
            latest_file = max(json_files, key=os.path.getctime)
            return latest_file
        
        def load_and_validate_results(file_path: Path) -> Optional[Dict[str, Any]]:
            """Load and validate benchmark results."""
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if not isinstance(data, dict):
                    print(f"ERROR: Results file {file_path} does not contain a valid JSON object")
                    return None
                
                if not validate_json_structure(data):
                    print(f"ERROR: Results file {file_path} has invalid structure")
                    print(f"Expected fields: name, run_id, parameters")
                    print(f"Found fields: {list(data.keys())}")
                    return None
                
                print(f"Successfully loaded results from: {file_path}")
                return data
                
            except json.JSONDecodeError as e:
                print(f"ERROR: Failed to parse JSON from {file_path}: {e}")
                return None
            except Exception as e:
                print(f"ERROR: Failed to load results from {file_path}: {e}")
                return None
        
        def check_performance_regressions(data: Dict[str, Any]) -> List[str]:
            """Check for performance regressions in the benchmark results."""
            regressions = []
            
            # Get metrics from various possible locations
            metrics = data.get('metrics', {})
            iteration_metrics = data.get('iteration_metrics', [])
            
            # Extract metrics from iteration results if main metrics are empty
            if not metrics and iteration_metrics:
                # Aggregate metrics from iterations
                all_iteration_metrics = {}
                for iter_result in iteration_metrics:
                    if isinstance(iter_result, dict) and 'metrics' in iter_result:
                        iter_metrics = iter_result['metrics']
                        for key, value in iter_metrics.items():
                            if key not in all_iteration_metrics:
                                all_iteration_metrics[key] = []
                            if isinstance(value, (int, float)):
                                all_iteration_metrics[key].append(value)
                
                # Calculate means for aggregated metrics
                for key, values in all_iteration_metrics.items():
                    if values:
                        metrics[key] = sum(values) / len(values)
            
            print(f"Available metrics: {list(metrics.keys())}")
            
            # Check benchmark duration (expect under 30 seconds for database benchmark)
            duration = extract_metric_value(metrics, 'duration_s', default=0.0)
            if duration > 30.0:
                regressions.append(f"Benchmark duration too long: {duration:.2f}s (expected < 30s)")
            
            # Check memory vs disk performance (expect memory to be faster)
            # Look for various possible speedup metric names
            speedup_metrics = [
                'disk_to_memory_speedup',
                'memory_vs_disk_speedup', 
                'speedup',
                'memory_speedup'
            ]
            
            speedup_found = False
            for speedup_key in speedup_metrics:
                speedup = extract_metric_value(metrics, speedup_key, default=0.0)
                if speedup > 0:
                    speedup_found = True
                    if speedup < 1.0:
                        regressions.append(f"Memory database not faster than disk: {speedup:.2f}x speedup (expected > 1.0x)")
                    break
            
            # Check for memory database performance (expect reasonable performance)
            memory_time = extract_metric_value(metrics, 'memory_time', default=0.0)
            if memory_time > 0 and memory_time > 20.0:  # Expect memory DB to be fast
                regressions.append(f"Memory database too slow: {memory_time:.2f}s (expected < 20s)")
            
            # Check disk database performance (baseline check)
            disk_time = extract_metric_value(metrics, 'disk_time', default=0.0)
            if disk_time > 0 and disk_time > 60.0:  # Expect disk DB to be reasonable
                regressions.append(f"Disk database too slow: {disk_time:.2f}s (expected < 60s)")
            
            # Check if we have any meaningful metrics at all
            if not metrics and not iteration_metrics:
                regressions.append("No performance metrics found in results")
            elif not speedup_found and not any(extract_metric_value(metrics, key) > 0 for key in ['memory_time', 'disk_time', 'duration_s']):
                regressions.append("No meaningful performance metrics found")
            
            return regressions
        
        # Main execution
        try:
            # Find and load the latest results
            latest_file = find_latest_results()
            if not latest_file:
                print("âŒ No benchmark results found - cannot check for regressions")
                sys.exit(1)
            
            data = load_and_validate_results(latest_file)
            if not data:
                print("âŒ Failed to load valid benchmark results")
                sys.exit(1)
            
            # Check for regressions
            regressions = check_performance_regressions(data)
            
            if regressions:
                print("ğŸš¨ PERFORMANCE REGRESSIONS DETECTED:")
                for regression in regressions:
                    print(f"  - {regression}")
                print("\nConsider investigating these performance issues before merging.")
                sys.exit(1)
            else:
                print("âœ… No significant performance regressions detected.")
                sys.exit(0)
                
        except Exception as e:
            print(f"âŒ Unexpected error during performance regression check: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
        EOF
        
    - name: Comment on PR with performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Find the latest benchmark results file
          const resultsDir = 'benchmarks/results/database_profiling';
          
          if (!fs.existsSync(resultsDir)) {
            console.log('Results directory does not exist');
            return;
          }
          
          const files = fs.readdirSync(resultsDir);
          const resultFiles = files.filter(f => f.endsWith('.json'));
          
          if (resultFiles.length === 0) {
            console.log('No benchmark results found');
            return;
          }
          
          // Get the most recent file by modification time
          const latestFile = resultFiles
            .map(f => ({
              name: f,
              mtime: fs.statSync(path.join(resultsDir, f)).mtime
            }))
            .sort((a, b) => b.mtime - a.mtime)[0].name;
          
          const resultPath = path.join(resultsDir, latestFile);
          
          try {
            const resultData = JSON.parse(fs.readFileSync(resultPath, 'utf8'));
            
            // Extract key metrics for display with robust handling
            const metrics = resultData.metrics || {};
            const iterationMetrics = resultData.iteration_metrics || [];
            
            // Helper function to safely extract metric values
            function extractMetricValue(metricsObj, key, defaultValue = 'N/A') {
              if (key in metricsObj) {
                const value = metricsObj[key];
                if (typeof value === 'object' && value !== null) {
                  if ('mean' in value) return value.mean.toFixed(2);
                  if ('value' in value) return value.value;
                  return JSON.stringify(value);
                }
                return value;
              }
              return defaultValue;
            }
            
            // Build summary with key performance metrics
            const summaryLines = [];
            
            // Duration metrics
            const duration = extractMetricValue(metrics, 'duration_s');
            if (duration !== 'N/A') {
              summaryLines.push(`Duration: ${duration}s`);
            }
            
            // Speedup metrics
            const speedupKeys = ['disk_to_memory_speedup', 'memory_vs_disk_speedup', 'speedup'];
            for (const key of speedupKeys) {
              const speedup = extractMetricValue(metrics, key);
              if (speedup !== 'N/A') {
                summaryLines.push(`Speedup (${key}): ${speedup}x`);
                break;
              }
            }
            
            // Individual timing metrics
            const memoryTime = extractMetricValue(metrics, 'memory_time');
            const diskTime = extractMetricValue(metrics, 'disk_time');
            if (memoryTime !== 'N/A') summaryLines.push(`Memory DB Time: ${memoryTime}s`);
            if (diskTime !== 'N/A') summaryLines.push(`Disk DB Time: ${diskTime}s`);
            
            // If no metrics in main object, try to extract from iterations
            if (summaryLines.length === 0 && iterationMetrics.length > 0) {
              summaryLines.push(`Iterations completed: ${iterationMetrics.length}`);
              const avgDuration = iterationMetrics.reduce((sum, iter) => {
                return sum + (iter.duration_s || 0);
              }, 0) / iterationMetrics.length;
              if (avgDuration > 0) {
                summaryLines.push(`Average iteration duration: ${avgDuration.toFixed(2)}s`);
              }
            }
            
            const summary = summaryLines.length > 0 ? summaryLines.join('\n') : 'No metrics available';
            
            // Create comment
            const comment = `## ğŸš€ Database Performance Baseline Results
          
          Database performance benchmark completed for this PR. Here are the key metrics:
          
          \`\`\`
          ${summary}
          \`\`\`
          
          **Performance Status**: âœ… All checks passed
          
          ğŸ“Š **Artifacts**: Complete benchmark results are available in the workflow artifacts.
          
          ğŸ” **Next Steps**: 
          - Review the performance metrics above
          - Download artifacts for detailed analysis if needed
          - Performance trends are tracked automatically over time
          
          ---
          *This comment is automatically generated by the Database Performance Baseline workflow.*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
            
          } catch (error) {
            console.log('Error processing benchmark results:', error.message);
            
            // Fallback comment if we can't parse the results
            const fallbackComment = `## ğŸš€ Database Performance Baseline Results
          
          Database performance benchmark completed for this PR.
          
          âš ï¸ **Note**: Results file found but could not be parsed. Check the workflow artifacts for detailed results.
          
          **Performance Status**: âœ… Benchmark completed (results parsing failed)
          
          ğŸ“Š **Artifacts**: Complete benchmark results are available in the workflow artifacts.
          
          ---
          *This comment is automatically generated by the Database Performance Baseline workflow.*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: fallbackComment
            });
          }
