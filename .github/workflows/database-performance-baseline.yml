name: Database Performance Baseline

on:
  pull_request:
    branches: [ main ]
    types: [ opened, synchronize, reopened ]
  push:
    branches: [ main ]

jobs:
  database-performance-baseline:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Create results directory
      run: |
        mkdir -p benchmarks/results/database_profiling
        
    - name: Run database performance benchmark
      run: |
        python -m benchmarks.run_benchmarks --spec benchmarks/specs/memory_db_baseline.yaml
        
    - name: Upload profiling results
      uses: actions/upload-artifact@v4
      with:
        name: database-profiling-results-${{ github.run_number }}
        path: benchmarks/results/database_profiling/
        retention-days: 30
        
    - name: Generate performance summary
      run: |
        echo "## Database Performance Baseline Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Latest Performance Metrics" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Extract key metrics from the latest benchmark results
        LATEST_RESULTS=$(ls -t benchmarks/results/database_profiling/memory_db_*.json | head -1)
        if [ -f "$LATEST_RESULTS" ]; then
          echo "```" >> $GITHUB_STEP_SUMMARY
          echo "Database Performance Results:" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          # Extract key metrics using jq if available, otherwise show file info
          if command -v jq &> /dev/null; then
            jq -r '.metrics | to_entries[] | "\(.key): \(.value)"' "$LATEST_RESULTS" >> $GITHUB_STEP_SUMMARY
          else
            echo "Results file: $(basename "$LATEST_RESULTS")" >> $GITHUB_STEP_SUMMARY
            echo "Check artifacts for detailed results" >> $GITHUB_STEP_SUMMARY
          fi
          echo "```" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Performance Artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Complete profiling results are available in the workflow artifacts" >> $GITHUB_STEP_SUMMARY
        echo "- Results include JSON, CSV, and summary report formats" >> $GITHUB_STEP_SUMMARY
        echo "- Use these results to track performance trends over time" >> $GITHUB_STEP_SUMMARY
        
    - name: Check for performance regressions
      run: |
        echo "Checking for significant performance regressions..."
        
        # Load the latest results and check against thresholds
        python3 << 'EOF'
        import json
        import glob
        import os
        from pathlib import Path
        
        # Find the latest JSON results file
        results_dir = Path("benchmarks/results/database_profiling")
        json_files = list(results_dir.glob("memory_db_*.json"))
        
        if not json_files:
            print("No benchmark results found")
            exit(0)
            
        latest_file = max(json_files, key=os.path.getctime)
        
        with open(latest_file, 'r') as f:
            data = json.load(f)
        
        metrics = data.get('metrics', {})
        regressions = []
        
        # Check overall performance metrics
        if 'duration_s' in metrics:
            duration = metrics['duration_s'].get('mean', 0)
            if duration > 10.0:  # Expect benchmark to complete in under 10 seconds
                regressions.append(f"Benchmark duration too long: {duration:.2f}s (expected < 10s)")
        
        # Check memory vs disk performance (expect memory to be faster)
        if 'memory_vs_disk_speedup' in metrics:
            speedup = metrics['memory_vs_disk_speedup']
            if speedup < 1.0:
                regressions.append(f"Memory database not faster than disk: {speedup:.2f}x speedup (expected > 1.0x)")
        
        # Check throughput metrics
        if 'operations_per_second' in metrics:
            ops_per_sec = metrics['operations_per_second']
            if ops_per_sec < 1000:  # Expect at least 1000 ops/sec
                regressions.append(f"Operations per second below threshold: {ops_per_sec:.0f} ops/s (expected > 1,000)")
        
        if regressions:
            print("üö® PERFORMANCE REGRESSIONS DETECTED:")
            for regression in regressions:
                print(f"  - {regression}")
            print("\nConsider investigating these performance issues before merging.")
            exit(1)
        else:
            print("‚úÖ No significant performance regressions detected.")
            exit(0)
        EOF
        
    - name: Comment on PR with performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Find the latest benchmark results file
          const resultsDir = 'benchmarks/results/database_profiling';
          const files = fs.readdirSync(resultsDir);
          const resultFiles = files.filter(f => f.startsWith('memory_db_') && f.endsWith('.json'));
          
          if (resultFiles.length === 0) {
            console.log('No benchmark results found');
            return;
          }
          
          // Get the most recent file
          const latestFile = resultFiles.sort().pop();
          const resultPath = path.join(resultsDir, latestFile);
          const resultData = JSON.parse(fs.readFileSync(resultPath, 'utf8'));
          
          // Extract key metrics for display
          const metrics = resultData.metrics || {};
          const summary = Object.entries(metrics)
            .map(([key, value]) => `${key}: ${typeof value === 'object' ? JSON.stringify(value) : value}`)
            .join('\n');
          
          // Create comment
          const comment = `## üöÄ Database Performance Baseline Results
          
          Database performance benchmark completed for this PR. Here are the key metrics:
          
          \`\`\`
          ${summary}
          \`\`\`
          
          **Performance Status**: ‚úÖ All checks passed
          
          üìä **Artifacts**: Complete benchmark results are available in the workflow artifacts.
          
          üîç **Next Steps**: 
          - Review the performance metrics above
          - Download artifacts for detailed analysis if needed
          - Performance trends are tracked automatically over time
          
          ---
          *This comment is automatically generated by the Database Performance Baseline workflow.*`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
