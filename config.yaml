# Redis configuration
redis:
  host: localhost
  port: 6379
  db: 0
  password: null
  decode_responses: true
  environment: default

# Unified DQN profiles and action configurations
# Define DQN learning profiles once and reuse them across actions
# Existing per-action legacy keys remain above for backward-compat and will be
# overridden at load time based on the unified format below.
dqn_profiles:
  default: &default_dqn
    learning_rate: 0.001
    gamma: 0.99
    epsilon_start: 1.0
    epsilon_min: 0.01
    epsilon_decay: 0.995
    memory_size: 10000
    batch_size: 32
    hidden_size: 64
    tau: 0.005
    target_update_freq: 100

  fast_learning: &fast_dqn
    <<: *default_dqn
    learning_rate: 0.005
    epsilon_decay: 0.99

# Action configurations (only specify what's different from the profile)
actions:
  move:
    profile: default
    costs: {base: -0.1}
    rewards: {approach_resource: 0.3}
    overrides: {retreat_penalty: -0.2}

  attack:
    profile: default
    costs: {base: -0.2}
    rewards: {success: 1.0, kill: 5.0}
    overrides: {defense_threshold: 0.3, defense_boost: 2.0}

  gather:
    profile: default
    costs: {base: -0.05}
    rewards: {success: 0.5, failure_penalty: -0.1}
    overrides:
      distance_penalty_factor: 0.1
      resource_threshold: 0.2
      competition_penalty: -0.2
      efficiency_bonus: 0.3
      max_wait_steps: 10

  share:
    profile: default
    costs: {base: -0.05}
    rewards: {success: 0.5, failure_penalty: -0.1}
    overrides:
      range: 30.0
      min_share_amount: 1
      max_share_amount: 5
      share_threshold: 0.3
      cooperation_bonus: 0.2
      altruism_factor: 1.2